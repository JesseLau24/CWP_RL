{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Create a directory to store S&P 500 stock data\n",
    "directory = \"SP500_10_25\"\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "\n",
    "# Retrieve the list of S&P 500 stocks from Wikipedia\n",
    "sp500_url = \"https://en.wikipedia.org/wiki/List_of_S%26P_500_companies\"\n",
    "table = pd.read_html(sp500_url)[0]\n",
    "tickers = table['Symbol'].tolist()\n",
    "\n",
    "# Add VIX and S&P 500 index (^VIX, ^GSPC)\n",
    "tickers.extend([\"^VIX\", \"^GSPC\"])\n",
    "\n",
    "# Fetch stock data and save as CSV\n",
    "for ticker in tickers:\n",
    "    print(f\"Fetching data for: {ticker}\")\n",
    "    stock_data = yf.download(ticker, start=\"2010-01-01\", end=\"2025-04-01\")\n",
    "    \n",
    "    # Check if the number of data points is less than 100 days\n",
    "    if len(stock_data) < 100:\n",
    "        print(f\"Skipping {ticker}, insufficient data: {len(stock_data)} days\")\n",
    "        continue  # Skip this stock and do not save data\n",
    "\n",
    "    # Keep only the required columns (Open, High, Low, Close, Volume)\n",
    "    stock_data = stock_data[['Open', 'High', 'Low', 'Close', 'Volume']]\n",
    "    \n",
    "    # Save data as CSV\n",
    "    stock_data.to_csv(f\"{directory}/{ticker}.csv\")\n",
    "    print(f\"Data for {ticker} has been saved\")\n",
    "\n",
    "print(\"All eligible stock data has been successfully saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Set storage path\n",
    "directory = \"SP500_10_25\"\n",
    "\n",
    "# Process each CSV file\n",
    "for ticker in os.listdir(directory):\n",
    "    if ticker.endswith(\".csv\"):\n",
    "        file_path = os.path.join(directory, ticker)\n",
    "        \n",
    "        # Read the CSV file\n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "        # Delete the second and third rows\n",
    "        df = df.drop([0, 1]).reset_index(drop=True)\n",
    "\n",
    "        # Remove rows containing NaN values\n",
    "        df = df.dropna()\n",
    "\n",
    "        # Rename the first column to 'Date'\n",
    "        df.columns.values[0] = 'Date'\n",
    "\n",
    "        # Save the modified file\n",
    "        df.to_csv(file_path, index=False)\n",
    "\n",
    "        print(f\"Processing complete: {ticker}\")\n",
    "\n",
    "print(\"ðŸŽ‰ All CSV files have been cleaned!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "directory = \"SP500_10_25\"\n",
    "\n",
    "# Get a list of all CSV files\n",
    "csv_files = [f for f in os.listdir(directory) if f.endswith(\".csv\")]\n",
    "\n",
    "# Initialize an empty list to store individual stock DataFrames\n",
    "dfs = []\n",
    "\n",
    "# Read and store each CSV file\n",
    "for file in csv_files:\n",
    "    stock_df = pd.read_csv(os.path.join(directory, file), parse_dates=[\"Date\"])\n",
    "    stock_df[\"Stock_ID\"] = file.replace(\".csv\", \"\")  # Add stock ticker as an identifier\n",
    "    dfs.append(stock_df)\n",
    "\n",
    "# Concatenate all stock DataFrames\n",
    "training_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Sort by Date first, then by Stock_ID\n",
    "training_df = training_df.sort_values(by=[\"Date\", \"Stock_ID\"]).reset_index(drop=True)\n",
    "\n",
    "# Reorder columns: Date â†’ Stock_ID â†’ Other Columns\n",
    "cols = [\"Date\", \"Stock_ID\"] + [col for col in training_df.columns if col not in [\"Date\", \"Stock_ID\"]]\n",
    "training_df = training_df[cols]\n",
    "\n",
    "# Create new directory\n",
    "directory = \"training\"\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "\n",
    "# Save merged data to a new CSV file\n",
    "training_csv_path = os.path.join(directory, \"10_25_merged_stocks.csv\")\n",
    "training_df.to_csv(training_csv_path, index=False)\n",
    "\n",
    "print(f\"Merging completed. Merged file saved as '{training_csv_path}'.\")\n",
    "\n",
    "training_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the merged dataset\n",
    "file_path = \"/home/jesse/Projects/CWP_RL/03_XGBoost_Return_Prediction/training/10_25_merged_stocks.csv\"\n",
    "df = pd.read_csv(file_path, parse_dates=[\"Date\"])\n",
    "\n",
    "# Ensure sorting by Date and Stock_ID\n",
    "df = df.sort_values(by=[\"Stock_ID\", \"Date\"]).reset_index(drop=True)\n",
    "\n",
    "# Function to calculate technical indicators\n",
    "def calculate_features(df):\n",
    "    \n",
    "    # Shift to avoid look-ahead bias (so this would be the next day's Return_1d)\n",
    "    df[\"Return_1d\"] = df.groupby(\"Stock_ID\")[\"Close\"].pct_change(1).shift(-1)\n",
    "    \n",
    "    # The rest of them remains the same\n",
    "    df[\"Return_5d\"] = df.groupby(\"Stock_ID\")['Close'].pct_change(5)\n",
    "    df[\"Return_10d\"] = df.groupby(\"Stock_ID\")['Close'].pct_change(10)\n",
    "    df[\"Return_50d\"] = df.groupby(\"Stock_ID\")['Close'].pct_change(50)\n",
    "    \n",
    "    # Rolling volatility\n",
    "    df[\"Volatility_5d\"] = df.groupby(\"Stock_ID\")[\"Return_1d\"].rolling(5).std().reset_index(level=0, drop=True)\n",
    "    df[\"Volatility_10d\"] = df.groupby(\"Stock_ID\")[\"Return_1d\"].rolling(10).std().reset_index(level=0, drop=True)\n",
    "    df[\"Volatility_20d\"] = df.groupby(\"Stock_ID\")[\"Return_1d\"].rolling(20).std().reset_index(level=0, drop=True)\n",
    "    \n",
    "    # Momentum indicators\n",
    "    df[\"SMA_10\"] = df.groupby(\"Stock_ID\")[\"Close\"].rolling(10).mean().reset_index(level=0, drop=True)\n",
    "    df[\"SMA_50\"] = df.groupby(\"Stock_ID\")[\"Close\"].rolling(50).mean().reset_index(level=0, drop=True)\n",
    "    df[\"SMA_200\"] = df.groupby(\"Stock_ID\")[\"Close\"].rolling(200).mean().reset_index(level=0, drop=True)\n",
    "    df[\"RSI_14\"] = 100 - (100 / (1 + df.groupby(\"Stock_ID\")[\"Return_1d\"].rolling(14).apply(lambda x: np.mean(x[x > 0]) / np.mean(-x[x < 0]) if np.mean(-x[x < 0]) != 0 else np.inf).reset_index(level=0, drop=True)))\n",
    "    \n",
    "    # Volume-based features\n",
    "    df[\"Volume_Change_5d\"] = df.groupby(\"Stock_ID\")[\"Volume\"].pct_change(5)\n",
    "    df[\"Volume_Change_10d\"] = df.groupby(\"Stock_ID\")[\"Volume\"].pct_change(10)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply feature calculations\n",
    "df = calculate_features(df)\n",
    "\n",
    "# Save the new dataset with features\n",
    "output_path = \"/home/jesse/Projects/CWP_RL/03_XGBoost_Return_Prediction/training/10_25_merged_stocks_features.csv\"\n",
    "df.to_csv(output_path, index=False)\n",
    "print(f\"Feature engineering complete. Saved to {output_path}\")\n",
    "\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on my previous experience, this is basically the most troublesome ones\n",
    "print(df[[\"Volume_Change_5d\", \"Volume_Change_10d\"]].describe(percentiles=[0.01, 0.25, 0.5, 0.75, 0.99]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace infinite values with NaN (so they can be dropped)\n",
    "df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# Drop rows with NaN values\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "\n",
    "# Save the cleaned dataset\n",
    "file_path = \"/home/jesse/Projects/CWP_RL/03_XGBoost_Return_Prediction/training/10_25_merged_stocks_features.csv\"\n",
    "df.to_csv(file_path, index=False)\n",
    "\n",
    "print(f\"Cleaned dataset saved as '{file_path}'.\")\n",
    "\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Double check again!\n",
    "print(df[[\"Volume_Change_5d\", \"Volume_Change_10d\"]].describe(percentiles=[0.01, 0.25, 0.5, 0.75, 0.99]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the merged dataset\n",
    "file_path = \"/home/jesse/Projects/CWP_RL/03_XGBoost_Return_Prediction/training/10_25_merged_stocks_features.csv\"\n",
    "df = pd.read_csv(file_path, parse_dates=[\"Date\"])\n",
    "\n",
    "# Define split dates\n",
    "train_end_date = \"2020-12-31\"\n",
    "valid_start_date = \"2021-01-01\"\n",
    "valid_end_date = \"2021-12-31\"\n",
    "test_start_date = \"2022-01-01\"\n",
    "test_end_date = \"2025-04-01\"\n",
    "\n",
    "# Split into training, validation, and testing sets\n",
    "train_df = df[df[\"Date\"] <= train_end_date]\n",
    "valid_df = df[(df[\"Date\"] >= valid_start_date) & (df[\"Date\"] <= valid_end_date)]\n",
    "test_df = df[(df[\"Date\"] >= test_start_date) & (df[\"Date\"] <= test_end_date)]\n",
    "\n",
    "# Save to CSV\n",
    "output_dir = \"/home/jesse/Projects/CWP_RL/03_XGBoost_Return_Prediction/training\"\n",
    "train_path = f\"{output_dir}/train_2010_2020.csv\"\n",
    "valid_path = f\"{output_dir}/valid_2021_2021.csv\"\n",
    "test_path = f\"{output_dir}/test_2022_2025.csv\"\n",
    "\n",
    "train_df.to_csv(train_path, index=False)\n",
    "valid_df.to_csv(valid_path, index=False)\n",
    "test_df.to_csv(test_path, index=False)\n",
    "\n",
    "print(f\"Training set saved: {train_path} ({len(train_df)} rows)\")\n",
    "print(f\"Validation set saved: {valid_path} ({len(valid_df)} rows)\")\n",
    "print(f\"Testing set saved: {test_path} ({len(test_df)} rows)\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xgboost",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
