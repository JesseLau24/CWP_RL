{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== Cell 1: Load Data ====\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "\n",
    "# Load datasets\n",
    "train_df = pd.read_csv('Training/train_2013_2020.csv', parse_dates=[\"Date\"])\n",
    "valid_df = pd.read_csv('Training/valid_2021_2022.csv', parse_dates=[\"Date\"])\n",
    "test_df = pd.read_csv('Training/test_2023_2025.csv', parse_dates=[\"Date\"])\n",
    "\n",
    "print(\"\\n✅ Dataframes loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== Cell 2: Preprocessing ====\n",
    "# Sort data and encode ticker\n",
    "for df in [train_df, valid_df, test_df]:\n",
    "    df.sort_values(by=[\"Date\", \"Ticker\"], inplace=True)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "train_df['Ticker'] = label_encoder.fit_transform(train_df['Ticker'])\n",
    "valid_df['Ticker'] = label_encoder.transform(valid_df['Ticker'])\n",
    "test_df['Ticker'] = label_encoder.transform(test_df['Ticker'])\n",
    "\n",
    "# Separate features and target\n",
    "def prepare_features_targets(df):\n",
    "    X = df.drop(columns=[\"Date\", \"5_day_return\"])\n",
    "    y = df['5_day_return'].values\n",
    "    return X, y\n",
    "\n",
    "X_train_df, y_train = prepare_features_targets(train_df)\n",
    "X_val_df, y_val = prepare_features_targets(valid_df)\n",
    "X_test_df, y_test = prepare_features_targets(test_df)\n",
    "\n",
    "# Normalize features\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train_df)\n",
    "X_val = scaler.transform(X_val_df)\n",
    "X_test = scaler.transform(X_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== Cell 3: Sequence Preparation ====\n",
    "def create_sequences(features, targets, window_size=60):\n",
    "    X, y = [], []\n",
    "    for i in range(window_size, len(features)):\n",
    "        X.append(features[i - window_size:i])\n",
    "        y.append(targets[i])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "X_train, y_train = create_sequences(X_train, y_train)\n",
    "X_val, y_val = create_sequences(X_val, y_val)\n",
    "X_test, y_test = create_sequences(X_test, y_test)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.float32).unsqueeze(1)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "print(f\"✅ Training shape: {X_train_tensor.shape}\")\n",
    "print(f\"✅ Validation shape: {X_val_tensor.shape}\")\n",
    "print(f\"✅ Test shape: {X_test_tensor.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== Cell 4: Model Definition ====\n",
    "import torch.nn as nn\n",
    "\n",
    "class StockLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes=[64, 32], fc_sizes=[16], dropout=0.0):\n",
    "        super(StockLSTM, self).__init__()\n",
    "\n",
    "        self.lstm_layers = nn.ModuleList()\n",
    "        in_size = input_size\n",
    "        for hidden_size in hidden_sizes:\n",
    "            self.lstm_layers.append(nn.LSTM(input_size=in_size, hidden_size=hidden_size, batch_first=True))\n",
    "            in_size = hidden_size\n",
    "\n",
    "        self.fc_layers = nn.ModuleList()\n",
    "        for fc_size in fc_sizes:\n",
    "            self.fc_layers.append(nn.Linear(in_size, fc_size))\n",
    "            in_size = fc_size\n",
    "\n",
    "        self.output_layer = nn.Linear(in_size, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for lstm in self.lstm_layers:\n",
    "            x, _ = lstm(x)\n",
    "        x = x[:, -1, :]  # Last time step\n",
    "        for fc in self.fc_layers:\n",
    "            x = self.relu(fc(x))\n",
    "            x = self.dropout(x)\n",
    "        return self.output_layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== Cell 5: Model Initialization ====\n",
    "input_size = X_train_tensor.shape[2]\n",
    "\n",
    "# Adjust the structure here:\n",
    "model = StockLSTM(\n",
    "    input_size,\n",
    "    hidden_sizes=[64, 32],  # LSTM layers: list of hidden sizes, one per layer\n",
    "    fc_sizes=[32],           # Fully connected layers: list of layer sizes\n",
    "    dropout=0.0                  # Dropout applied after each FC layer (not LSTM layers)\n",
    ")\n",
    "\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== Cell 6: Training ====\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "print(f\"Training on device: {device}\")\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "num_epochs = 1500\n",
    "train_losses, val_losses = [], []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # train_loss += loss.item()\n",
    "        train_loss += loss.detach().item() # to avoid converting a tensor with requires_grad=True, which is a UserWarning I am trying to avoid\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in val_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    train_losses.append(train_loss / len(train_loader))\n",
    "    val_losses.append(val_loss / len(val_loader))\n",
    "\n",
    "    if (epoch + 1) % 10 == 0 or epoch == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] - Train Loss: {train_losses[-1]:.4f}, Val Loss: {val_losses[-1]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== Cell 7: Plotting ====\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title(\"Training vs Validation Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== Cell 8: Evaluation ====\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "model.eval()\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "y_pred, y_true = [], []\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        outputs = model(X_batch)\n",
    "        y_pred.extend(outputs.cpu().numpy())\n",
    "        y_true.extend(y_batch.cpu().numpy())\n",
    "\n",
    "y_pred = np.array(y_pred).flatten()\n",
    "y_true = np.array(y_true).flatten()\n",
    "\n",
    "mae = mean_absolute_error(y_true, y_pred)\n",
    "mse = mean_squared_error(y_true, y_pred)\n",
    "r2 = r2_score(y_true, y_pred)\n",
    "\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
    "print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
    "print(f\"R-squared: {r2:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_nightly",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
